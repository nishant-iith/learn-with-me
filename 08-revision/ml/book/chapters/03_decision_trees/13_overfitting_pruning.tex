\chapter{Overfitting and Pruning}
\label{chap:pruning}

\section{The Tree's Achilles Heel: Overfitting}
Decision Trees are aggressive learners. If left unchecked, they will split the data until every single leaf node is "Pure".
\begin{itemize}
    \item Imagine a leaf node containing just \textbf{1 person} who is an outlier (e.g., a smoker who lived to 100).
    \item The tree learns a rule: "If you smoke and live in this specific zip code $\rightarrow$ You live to 100".
    \item This is \textbf{Overfitting}. The tree memorizes the training noise.
\end{itemize}

\begin{definition}
\textbf{Pruning}: The technique of reducing the size of a decision tree by removing sections that provide little power to classify instances. It allows the tree to generalize better.
\end{definition}

\section{Pre-Pruning (Early Stopping)}
We stop the tree \textit{while} it is growing. This is done using Hyperparameters.

\begin{itemize}
    \item \texttt{max\_depth}: Constraint the tree height (e.g., max 5 levels).
    \item \texttt{min\_samples\_split}: "Don't split this node unless it has at least 50 people".
    \item \texttt{min\_samples\_leaf}: "Don't create a leaf unless it has at least 10 people". (Crucial for smoothing).
\end{itemize}

\section{Post-Pruning (Cost Complexity Pruning)}
Pre-pruning is basically guessing parameters. A more mathematical approach is to let the tree grow fully (to 100\% purity), and then cut off the "weakest" branches.
Scikit-Learn uses **Minimal Cost-Complexity Pruning**.

\subsection{The Math: Cost Function}
We assign a "Cost" to a tree $T$:
\begin{equation}
    R_\alpha(T) = R(T) + \alpha |T|
\end{equation}
Where:
\begin{itemize}
    \item $R(T)$: The Total Impurity (Error) of the tree.
    \item $|T|$: The Number of Leaf Nodes (Complexity).
    \item $\alpha$ (Alpha): A tuning parameter that penalizes complexity.
\end{itemize}

\begin{itemize}
    \item If $\alpha = 0$: We get the full, complex tree (Low Error, High Complexity).
    \item If $\alpha$ is large: The penalty for having leaves is huge. The tree shrinks to a stump.
\end{itemize}
We use Cross-Validation to find the optimal $\alpha$ that gives the best Test Accuracy.

% ========================================
% SECTION 4: IMPLEMENTATION
% ========================================
\section{Implementation in Python (Post-Pruning)}
\begin{lstlisting}[language=Python, caption=Cost Complexity Pruning with Scikit-Learn]
from sklearn.datasets import load_breast_cancer
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

# 1. Load Data
X, y = load_breast_cancer(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)

# 2. Get Pruning Path (Alphas)
clf = DecisionTreeClassifier(random_state=0)
path = clf.cost_complexity_pruning_path(X_train, y_train)
ccp_alphas, impurities = path.ccp_alphas, path.impurities

# 3. Train potential trees for each alpha
clfs = []
for ccp_alpha in ccp_alphas:
    clf = DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha)
    clf.fit(X_train, y_train)
    clfs.append(clf)

# 4. Compare Accuracy to find Best Alpha
train_scores = [clf.score(X_train, y_train) for clf in clfs]
test_scores = [clf.score(X_test, y_test) for clf in clfs]

# Pick alpha with highest Test Score (simplest code visualization)
best_id = test_scores.index(max(test_scores))
print(f"Best Alpha: {ccp_alphas[best_id]:.4f} with Accuracy: {test_scores[best_id]:.2f}")
\end{lstlisting}

\section{HOTS Questions}
\textbf{Q1: Why is a single Decision Tree called a "Weak Learner"?}
\\ \textit{Answer}: It suffers from High Variance. A small change in the training data (changing one point) can result in a completely different tree structure. This instability is why we combine hundreds of trees to form a **Random Forest**.

\textbf{Q2: Which pruning method is better?}
\\ \textit{Answer}: Post-Pruning is generally robust because it sees the "full picture" before cutting. Pre-pruning might stop too early (The "Horizon Effect")â€”it might not split a node because the immediate gain is low, missing a massive gain one step deeper.
