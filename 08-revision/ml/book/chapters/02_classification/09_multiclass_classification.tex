\chapter{Multiclass Classification}
\label{chap:multiclass}

% ========================================
% SECTION 1: INTRODUCTION
% ========================================
\section{Introduction}
So far, we have dealt with Binary classification (2 classes: 0 or 1).
What if we have more than two classes? (e.g., classifying a fruit as Apple, Banana, or Orange).
Logistic Regression with Sigmoid is natively binary. How do we extend it?

% ========================================
% SECTION 2: ONE-VS-REST
% ========================================
\section{Strategy 1: One-Vs-Rest (OvR)}
\begin{definition}
\textbf{One-Vs-Rest (OvR)}: A strategy that breaks a K-class problem into K binary classification problems. For each class, we train a separate model that distinguishes it from all other classes combined.
\end{definition}

For 3 classes (A, B, C), we train 3 models:
\begin{enumerate}
    \item \textbf{Model 1}: Is it A? (A vs Not-A, i.e., B $\cup$ C)
    \item \textbf{Model 2}: Is it B? (B vs Not-B)
    \item \textbf{Model 3}: Is it C? (C vs Not-C)
\end{enumerate}

For a new data point, we run all 3 models. The one with the \textbf{highest probability} wins.

\textbf{Pros}: Simple to implement. Works with any binary classifier.
\textbf{Cons}: Trains $K$ separate models. Class imbalance issues.

% ========================================
% SECTION 3: SOFTMAX REGRESSION
% ========================================
\section{Strategy 2: Softmax Regression (Multinomial Logistic)}
Instead of training multiple models, we generalize the Sigmoid function to multiple dimensions.

\begin{definition}
\textbf{Softmax Function}: A generalization of Sigmoid to $K$ classes. It converts $K$ raw scores (logits) into a probability distribution.
\begin{equation}
    P(y=j|x) = \frac{e^{z_j}}{\sum_{k=1}^{K} e^{z_k}}
\end{equation}
\end{definition}

\textbf{Key Property}: The probabilities of all classes sum to 1.
\begin{itemize}
    \item Example output: [Apple: 0.7, Banana: 0.2, Orange: 0.1].
    \item Prediction: Apple (highest probability).
\end{itemize}

% ========================================
% SECTION 4: EVALUATING MULTICLASS MODELS
% ========================================
\section{Evaluating Multiclass Models}
Evaluating a 2-class model is easy. Evaluating a 3+ class model requires us to look deeper.
We extend the Confusion Matrix from $2 \times 2$ to $N \times N$.

\subsection{The Confusion Matrix ($N \times N$)}
Consider a model classifying images into \textbf{Dog}, \textbf{Cat}, or \textbf{Rabbit}.

\begin{table}[htbp]
    \centering
    \begin{tabular}{|l|c|c|c||c|}
    \hline
    & \textbf{Pred Dog} & \textbf{Pred Cat} & \textbf{Pred Rabbit} & \textbf{Total Actual} \\ \hline
    \textbf{Actual Dog} & \textbf{25 (TP)} & 5 & 10 & 40 \\ \hline
    \textbf{Actual Cat} & 0 & \textbf{30 (TP)} & 4 & 34 \\ \hline
    \textbf{Actual Rabbit} & 4 & 10 & \textbf{20 (TP)} & 34 \\ \hline\hline
    \textbf{Total Pred} & 29 & 45 & 34 & \textbf{108} \\ \hline
    \end{tabular}
    \caption{Confusion Matrix Example. Diagonal elements are correct predictions.}
    \label{tab:dog_cat_rabbit}
\end{table}

\subsection{Precision, Recall, and F1 (Per Class)}
We calculate metrics for each class individually by treating it as ``This Class'' vs ``Everything Else''.

\begin{itemize}
    \item \textbf{Precision (Col Exactness)}: $TP / \text{Total Predicted}$.
    \begin{itemize}
        \item Dog: $25 / 29 = \mathbf{0.86}$.
        \item Cat: $30 / 45 = \mathbf{0.66}$.
        \item Rabbit: $20 / 34 = \mathbf{0.58}$.
    \end{itemize}
    
    \item \textbf{Recall (Row Completeness)}: $TP / \text{Total Actual}$.
    \begin{itemize}
        \item Dog: $25 / 40 = \mathbf{0.62}$.
        \item Cat: $30 / 34 = \mathbf{0.88}$.
        \item Rabbit: $20 / 34 = \mathbf{0.58}$.
    \end{itemize}
    
    \item \textbf{F1-Score}: Harmonic mean of Precision and Recall.
    \begin{itemize}
        \item Dog: $2(0.86 \cdot 0.62)/(0.86+0.62) = \mathbf{0.72}$.
        \item Cat: $\mathbf{0.75}$. Rabbit: $\mathbf{0.58}$.
    \end{itemize}
\end{itemize}

\subsection{Global Metrics (Averaging)}
How do we summarize these 3 numbers into one?

\begin{enumerate}
    \item \textbf{Macro Average}: Simple average. Treats every class equally (even if small).
    $$ \text{Macro P} = \frac{0.86 + 0.66 + 0.58}{3} = \mathbf{0.70} $$
    \textit{Use when minority classes (e.g., Rare Disease) are important.}
    
    \item \textbf{Weighted Average}: Weights by class size (Support).
    $$ \text{Weighted P} = \frac{40(0.86) + 34(0.66) + 34(0.58)}{108} \approx \mathbf{0.71} $$
    \textit{Use when you want to look good on the majority classes.}
\end{enumerate}

% ========================================
% SECTION 5: CATEGORICAL CROSS ENTROPY
% ========================================
\section{Loss Function: Categorical Cross Entropy}
\begin{definition}
\textbf{Categorical Cross Entropy}: The generalization of Binary Cross Entropy to multiple classes.
\begin{equation}
    J = - \sum_{i=1}^{n} \sum_{j=1}^{K} y_{ij} \log(\hat{y}_{ij})
\end{equation}
\end{definition}
Where $y_{ij}$ is 1 if sample $i$ belongs to class $j$, else 0 (one-hot encoding).

% ========================================
% SECTION 5: CODE IMPLEMENTATION
% ========================================
\section{Implementation in Python}
Let us implement a classifier for the famous Iris dataset (3 flower species).

\begin{lstlisting}[language=Python, caption=Multiclass Classification with Softmax]
from sklearn.datasets import load_iris
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report

# ============================================
# STEP 1: Load Data
# ============================================
data = load_iris()
X, y = data.data, data.target
# Classes: 0 (Setosa), 1 (Versicolor), 2 (Virginica)

# ============================================
# STEP 2: Split and Scale
# ============================================
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)
scaler = StandardScaler()
X_train_s = scaler.fit_transform(X_train)
X_test_s = scaler.transform(X_test)

# ============================================
# STEP 3: Train Softmax Model
# ============================================
# multi_class='multinomial' uses Softmax
model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=200)
model.fit(X_train_s, y_train)

# ============================================
# STEP 4: Evaluate
# ============================================
y_pred = model.predict(X_test_s)
print(classification_report(y_test, y_pred, target_names=data.target_names))
\end{lstlisting}

% ========================================
% SECTION 6: KEY TAKEAWAYS
% ========================================
\section{Key Takeaways}
\begin{enumerate}
    \item \textbf{Binary vs Multiclass}: Binary uses Sigmoid (2 classes). Multiclass uses Softmax ($K$ classes).
    \item \textbf{OvR}: Trains $K$ binary models. Simple but can be slow.
    \item \textbf{Softmax}: Single model, outputs a probability distribution over all $K$ classes.
    \item \textbf{Loss}: Categorical Cross Entropy generalizes Binary Cross Entropy.
\end{enumerate}

% ========================================
% SECTION 7: HOTS QUESTIONS
% ========================================
\section{HOTS: Interview Questions}
\textbf{Q1: What is the difference between One-Vs-Rest and One-Vs-One?}
\begin{itemize}
    \item \textbf{OvR}: $K$ models. Each model is Class\_j vs All Others.
    \item \textbf{OvO}: $\binom{K}{2} = K(K-1)/2$ models. Each model is Class\_i vs Class\_j. More models, but each is simpler.
\end{itemize}

\textbf{Q2: When would Softmax fail?}
\begin{itemize}
    \item Softmax assumes classes are \textbf{mutually exclusive}. If an image can be both ``Dog'' and ``Brown'', Softmax is inappropriate. Use multiple Sigmoid outputs instead (Multi-Label Classification).
\end{itemize}
