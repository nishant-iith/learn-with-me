\chapter{Practical Implementation: Naive Bayes}
\label{chap:nb_practical}

\section{Implementation in Python (Scikit-Learn)}
We will use \textbf{GaussianNB} for the Iris dataset and \textbf{MultinomialNB} for Text Data.

\subsection{1. Gaussian Naive Bayes (Iris Data)}
\begin{lstlisting}[language=Python, caption=GaussianNB for Iris]
from sklearn.datasets import load_iris
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Load Data
data = load_iris()
X, y = data.data, data.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train GaussianNB
# Note: No Scaling required strictly, but good practice.
gnb = GaussianNB()
gnb.fit(X_train, y_train)

# Predict
y_pred = gnb.predict(X_test)
print(f"Accuracy: {accuracy_score(y_test, y_pred):.2f}")
\end{lstlisting}

\subsection{2. Text Classification (Spam Detection)}
For text, we first need to convert strings into numbers (Vectorization).

\begin{lstlisting}[language=Python, caption=Spam Filter with MultinomialNB]
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import Pipeline

# Sample Data
emails = [
    "Free money now", 
    "Hi mom, how are you?", 
    "Win a free lottery", 
    "Meeting correctly scheduled"
]
labels = [1, 0, 1, 0] # 1=Spam, 0=Ham

# Pipeline: Bag of Words -> Naive Bayes
model = Pipeline([
    ('vectorizer', CountVectorizer()),  # Converts text to counts
    ('classifier', MultinomialNB())     # Expects counts
])

model.fit(emails, labels)

# Predict new email
new_email = ["Free lottery for you"]
print(f"Prediction: {model.predict(new_email)}") # Output: [1] (Spam)
\end{lstlisting}

\section{When to use Naive Bayes?}
\begin{enumerate}
    \item \textbf{Text Classification}: It is the baseline Gold Standard for NLP (Sentinel Analysis, Spam Filtering) before Deep Learning.
    \item \textbf{Speed}: It is blazing fast (O(N)). Excellent for real-time systems.
    \item \textbf{Small Data}: Works surprisingly well even with little training data compared to complex models.
    \item \textbf{High Dimensions}: Ironically, the Curse of Dimensionality hurts distance-based models (KNN, SVM), but Naive Bayes handles high-dimensional text vectors efficiently.
\end{enumerate}

\section{HOTS: Interview Questions}
\textbf{Q1: Why do we not scale data for Multinomial Naive Bayes?}
\begin{itemize}
    \item MultinomialNB strictly expects integer counts.
    \item StandardScaling transforms data to floats (and potentially negatives), which violates the assumption of the frequency distribution.
\end{itemize}

\textbf{Q2: What is the difference between Discriminative and Generative models?}
\begin{itemize}
    \item \textbf{Discriminative} (LogReg, SVM): Learns the boundary $P(y|x)$ directly. "Discriminate between Dog and Cat".
    \item \textbf{Generative} (Naive Bayes): Learns the distribution of each class $P(x|y)$ and $P(y)$. "Generate a drawing of a Dog, generate a drawing of a Cat, and see which one matches the input".
\end{itemize}
